---
title: "Basic Data Wrangling"
author: "Niels Hanson"
date: "October 30, 2014"
output:
  html_document:
    keep_md: yes
    theme: readable
    toc: yes
---

Pourpose of this document is to summarize some basic data wrangling tasks that commonly occuring with metagenomic datasets in the Hallam Lab.

## Obtaining the GitHub Repo:

* download the repo by clicking the 'Download as Zip' on the [main page](www.github.com/hallamlab/data_wrangling)
* or by doing a `git clone` from the command line if git is installed

```
git clone https://github.com/hallamlab/data_wrangling.git
```

## Basic Unix Tips

* typing commands on the command-line can be a chore, but remember the following shortcuts:
    * `TAB`: auto-completes directories and filenames
    * `CTRL + a`: brings you to the front of the command you are typing
    * `CTRL + e`: brings you to the end
* setup the `META` key in Terminal to jump around commands by 'words' instead of spaces:
    * Terminal > Preferences
    * Click on the 'Keyboard' Tab
    * Check the 'Use option key as meta key' checkbox
    * now you can jump around your command with:
        * `OPTION + f`: move forward a word
        * `OPTION + b`: more backward a word
* personally I think the defaults for the shell are too small for detailed work:
    * increase the font size
    * pick a good highlighting theme:
        * [TomorrowNight](examples/unix/TomorrowNight.terminal)
        * [SolarizedLight](examples/unix/SolarizedLight.terminal)
        * [SolarizedDark](examples/unix/SolarizedDark.terminal)
        * [Monokai](examples/unix/Monokai.terminal)
    
## Unix Shell Commands: grep, awk, sed, and others

Some very-simple first-pass analyses can be performed with common Unix commands. Although there are others, many analysis and sanity checking tasks can be accomplished by the following commands:

* `grep`: `g`eneral `r`egular `e`xpression `p` program -- at its simplist a program that can **find text patterns** in the **names and contents of files**.
* `sed`: `s`tream `ed`itor -- a general pourpose program to modify a stream of text, which generally is a file or program output
* `awk`: (pronounced 'auk' or 'ach') is a **basic text processing** and **report generating language** developed by Alfred `a`ho, Peter `w`einberger, and Brian `k`ernighan at Bell Labs in the 1970s

### When to use?

Lets discuss some advantages and disadvantages and when these commands (by themselves) are a good option.

#### Advantages 

* these commands are basically found on every Unix environment
* fairly efficient and low-level and so will scale to fairly large files
* excellent for making back-of-the-envelope kinds of calculations to make sure files are properly formatted, the correct size, simple 'one-off' analyses

#### Disadvantages

* small isocyncracies in system experience needed to know command's behavior
* small mistakes can lead to huge headaches permenant data loss
* many commands have their GNU open-source sister program (e.g., `gawk` has different behavior than `awk`, but not always installed on every system) 
* behavior changes depending on version installed on OS (annoying)
* unless process is scripted (see next section) and documented carefully, complicated commands can be very diffcult to follow and reproduce (best to keep things simple)

### Examples

Lets go through an number of examples.

* if you're following along in the terminal, change directory `cd` to the `examples/unix_commands/` directory

```
cd examples/unix/
```

#### grep

The `grep` is generally used to identify and print lines in files that match a particular pattern. The basic command has the form:

```
grep 'pattern' file
```

* Counting the number of sequences in a fasta file
    * the `-c` option means 'count'
    * this will return he number of times the `'>'` pattern is observed in the fasta file
    * if the file is well-formatted, this will accurately tell you how many sequences there are in the file

```
grep -c '>' fastas/GUAB.fasta
9948
```

* this can also be done to every file that matches a particular file-name pattern in a directory with the **unix glob operator** (e.g., `*.fasta`)

```
grep -c '>' fastas/*.fasta
```

* count the number of sequences in all files in all subdirectories

```
grep ">" -c -r *
fastas/.DS_Store:0
fastas/GUAB.fasta:9948
fastas/GUAC.fasta:8850
fastas/INX043_RawGSCdata_min2000.fasta.NR00314_J24.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_F15.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_J19.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NR0031_K08.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR021_C16.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR044_H13.fasta:1
fastas/sub_dir2/NapDC_July06_2011_trimmed.fasta:15360
fastas/TOLDC_Feb22_2011_trimmed.fasta:3072
```

* but, if we were only interested in the `fasta` files use the `--include '.fasta'` option

```
grep -c -r --include '*.fasta*' '>' *
fastas/GUAB.fasta:9948
fastas/GUAC.fasta:8850
fastas/INX043_RawGSCdata_min2000.fasta.NR00314_J24.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_F15.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_J19.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NR0031_K08.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR021_C16.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR044_H13.fasta:1
fastas/sub_dir2/NapDC_July06_2011_trimmed.fasta:15360
fastas/TOLDC_Feb22_2011_trimmed.fasta:3072
```

* how about if were only interested in files that had `INX043` and were `.fasta` files anywhere in the folder `fastas/`
   * use the anycharacter glob (`*`) to select text on both sides

```
grep -c -r --include '*INX043*.fasta*' '>' *
fastas/INX043_RawGSCdata_min2000.fasta.NR00314_J24.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_F15.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_J19.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NR0031_K08.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR021_C16.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR044_H13.fasta:1
```

* and if I just wanted to look into directories that had the pattern `sub_dir*`

```
grep -c '>' fastas/sub_dir*/*fasta
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_F15.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NO00111_J19.fasta:1
fastas/sub_dir1/INX043_RawGSCdata_min2000.fasta.NR0031_K08.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR021_C16.fasta:1
fastas/sub_dir2/INX043_RawGSCdata_min2000.fasta.SCR044_H13.fasta:1
fastas/sub_dir2/NapDC_July06_2011_trimmed.fasta:15360
```

* there is also an `--exclude` to do the opposite of `--include`
    * here we look for `'>'` for all files in all subdirectories that do not end in `fasta`

```
grep -c -r --exclude '*.fasta' '>' *
command_list.sh:6
fastas/.DS_Store:0
```

##### Unix globs

* Note that there is quite a bit of power to these Unix 'glob' operators:
    * `*`: means any number of characters
        * `beginning*end`: selects files with a particular `beginning` and `end` patterns
        * `beginning*middle*end`: selects files with particular `beginning`, `middle`, and `end` text
        * `sample*/blast_results/*parsed_blast.txt`: matches all `parsed_blast.txt` files that can be found in sub-directories `blast_results/` below directries starting with `sample*`
    * `?`: matches exactly one unknown character
        * `?at`: matches `fat`, `cat`, `hat`, `sat`, etc.
    * `[]`: specifies a range or set of characters to match
        * `[BC]at`: matches `Bat` or `Cat` but not `bat` or `cat`
        * `sample_[0-9][0-9]`: matches `sample_01`, `sample_02`, etc.
    * `\`: 'escapes' the following character (where you actually want to match it)
        * `\[*proteobacteria*\]: matches text that have RefSeq-style taxonomy in square brackets
* the linux command `ls` can be used to list the set of files that are hit by a glob pattern
    * this is very important when writing critical commands like 'move' (`mv`) or 'remove' (`rm`)
        * everyone eventually burns themselves, its a fact of life while working on with the Unix command line
* *Note: Unix globs are not as sophisicated as full blown Perl or grep-based regular expressions. They don't have a complete feature set. Some things just can not be done with command line globs*
* To enable interpreting of regular expression in `grep` use the `-E` option

##### Regular Expressions

* Regular expressions are a model of text patterns and allow you to match much more complicated strings. They are much more advanced than Unix Globs, but look very simmilar. A fact which can mess you up.
* Operators:
    * `.`: Any character except end-of-line (`\n`)
    * `[A-z0-9]`: Any alpha-numeric character
        * [!abc]` or `[^a-c]`: negation of set (i.e., anything but abc) 
    * `\w`: any word
    * `\W`: any non-word
    * `^`: beginning of line
    * `$`: end of line
* Operations:
    * `()`: group selection - anything you want to get at later
    * `*`: **zero or more** of the previous pattern
    * `+`: **one or more** of the previous pattern
    * `?`: **zero or one** of the previous pattern

#### grep (con't)

* use glob patterns and to count the number of lines that match `Pseudomonas` in the Hawaii-ocean Time Series RefSeq LAST output (truncated data to fit on GitHub).

```
grep -c 'Pseudomonas' HOT_fosmids_small/*/blast_results/*RefSeq*.LASTout.parsed.txt
HOT_fosmids_small/below_euphotic_200m/blast_results/below_euphotic_200m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:99
HOT_fosmids_small/chlorophyllmax_130m/blast_results/chlorophyllmax_130m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:153
HOT_fosmids_small/deepabyss_4000m/blast_results/deepabyss_4000m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:208
HOT_fosmids_small/omz_770m/blast_results/omz_770m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:182
HOT_fosmids_small/upper_euphotic_10m/blast_results/upper_euphotic_10m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:167
HOT_fosmids_small/upper_euphotic_70m/blast_results/upper_euphotic_70m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:193
HOT_fosmids_small/uppermesopelagic_500m/blast_results/uppermesopelagic_500m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:190
```

* dropping the `-c`, `grep` will just print the matching line
    * match all fasta headers that end in `b1`

```
grep '>*b1' fastas/GUAB.fasta
>GUAB5590.b1
>GUAB3843.b1
>GUAB6002.b1
>GUAB6573.b1
```

* the `-n` option will print out the line number where the match was found
    if more than one file being searched the filename will be printed as well

```
grep -n '>*b1' fastas/GUAB.fasta
1:>GUAB5590.b1
3:>GUAB3843.b1
7:>GUAB6002.b1
11:>GUAB6573.b1
...
```

* if the fasta file format is well-formatted (contains the sequence only on one line after matching header), then you can use the 'after' `-A` flag to print a number of lines after the match.
    * *Note: there are also 'before' `-B` and 'context' `-C` flags that print out lines before the match and on both sides, respectively.* 
    * here we use it to get the sequence along with header patterns that end in `.b1`, because we know that it is the first line below in a well-formatted `.fasta` file
        * *Note: this won't work for fasta files where the sequence in on multiple lines*

```
grep -A 1 '>*b1' fastas/GUAB.fasta
>GUAB5590.b1
ATCGAGTGTGTTCTTTTGCGCGATGGTATTCGACGTACCGTTTGCATTTCATCACAAGTCGGCTGTGCAATGGGGTGTGTATTCTGTGCAAGCGGTCTTGATGGAGTTATCCGAAATTTGACAACCGGTGAGATCATCGAGCAGTTATTACGACTCACTCGTTTACTCCCAACAGAAGAACGACTAAGTCATATTGTTGTCATGGGAATGGGTGAACCACTAGCAAACCTCGATCGTTTATTACCTGCTCTTGCAATCGCACAAAGTCCTGAGGGACTTGGTATATCTCAACGACGAATCACTATTTCAACTGTCGGGTTGCCATCGGCAATTGATCGGCTGTGTCAAGAAAATCCTGGATATCATCTTGCCGTATCACTCCATGCCGCTGACGATCCACTCCGAACAAAACTCGTTCCAGTCAATAAGTCGATCGGAGTTCATGCCATTTTAGCCGCAGCTGACCGATACTGGGAAACATCTGGCCGACGACTTACTTTCGAATATGTCTTACTCGGAAATCTTAATGATTCTCCAGATCATGCCCGTTCGTTAGCTCGGTTTATCGGCAAACGTGCAGCGCTCGTAAACATTATTCCGTACAACACAGTCGATGGCCTACCGTGGGAAGAGCCTACTGACATTTCTCGTGAACGATTTCTTGATGTCCTTTCGAATGCTGGCGTGAATGTTCAGACTCGAAAAAGGAG
--
>GUAB3843.b1
CTGGGTGGCAGCGGTGTAGAGAAAATAGTTGAACTTTCGCTTTTGCCAGATGAAAAAGTGGCATTCAATAAAAGTATCGATGCAGTTCGGGAACTTGTTGGCGCAATGGAGAGCCTGACGCCATAACAGAAGCCCCCCAGTCCTCCCAGCCTCCCTCTGTCGGCTAGTTTCGCGAAAGCTCTTCATTTTCACACGGAAATGCTGCCATTGCCCGGAGTGTGTGACTGCGTTAGAAACCTCCGACGCATTGCCATAAGTCAAATCTCTTGTGGCAATGCGTTAGCAATCACGCTGTGTATTTTTGCAAAAGGTTACGCGGAGGATCTGGAATGGTGCATCAGTCGACAGTATTTGATTATGAATTCAATCATTGTTCACCAAGCAGTAGAGGCTTTTTGTGCAACGAGGTGAATCTCCCTTCGAGTAAACCAGAGCAACTGTTTTTACCACGAGGGTATGAGTCGGGATACGACTACCCACTCCTCGTGTGGCTTCCGCAGTCAGACGATGCTCACTTTGACTTAGGTCGCACCATGATGCGAATGAGTTTAAGGAATTACATTGCAGTAGTACCTGCTGTCACGTCTGATTTGGAGAGTTGTTTTGAAGCTATTGACGGAATAATGAGTCAATATAGCGTTCACTCTCGCAGATTTTATCTTATTGGTGTTGAGGAAGGTGGAGAGAATGCCTTTCGTTATGCTTGCCAAAATC
...
```

* we can now use 'write to file' operator `>` to safe this output to separate `.fasta` file
* whatever is print to the terminal (standard out) will be saved in the file `my_fasta.fasta`

```
grep -A 1  '>*b1' fastas/GUAB.fasta > my_fasta.fasta
```

* we can also append all the `g1` sequences to an existing file with `>>`

```
grep -A 1  '>*g1' fastas/GUAB.fasta >> my_fasta.fasta
```

* *Note: this isn't totally correct, because `grep` puts `--` characters between matches, which could cause fasta parsers to choke on the input. Solving this problem actually takes us into our next section which is on the Uinx 'Sequence Editor' program `sed`*

#### Sed

`Sed`, or the stream editor program is used to do precicely that, edit a stream of text. So this can be used to modify a string of text, usually using **regular expresisons**:

```
sed [options] commands [file-to-edit]
```

Basically `sed` reads in a file, performs editing operations on it and then prints it to the screen. If we give it no commands it just prints the text, akin to the `cat` program. You can 

```
sed '' command_list.sh
```

The biggest use of `sed` is to replace one some text with another piece of text. This has the famous 3-slashes format: `s/old_word/new_word/`

* here for example this goes through the command list file and replaces all instances of `grep` to `echo`.
* now might be a good time to also introduce the pipe operator `|` this takes the output (what is printed to the screen) of one program and feeds it as input to another.
   * many Unix commands can be created in this manner by 'chaining' commands together
   * here I chain the results of `sed` to another program called `head` which, by default, prints the first six lines of a file

```
sed 's/grep/echo/' command_list.sh | head
#!/bin/bash

# Command List for talk

## Unix Shell Commands

# echo counting
echo -c '>' fastas/GUAB.fasta
echo -c '>' fastas/*.fasta
echo ">" -c -r *
```

* lets correct our fasta file we create by removing those `--` characters:

```
grep -A 1  '>*b1' fastas/GUAB.fasta | sed 's/\-\-//'g > my_fasta.fasta
grep -A 1  '>*g1' fastas/GUAB.fasta | sed 's/\-\-//'g >> my_fasta.fasta
```

* alternatively use the `<` character to feed `my_fasta.fasta` into `sed` as input

```
sed < my_fasta.fasta 's/\-\-//'g > my_fasta_clean.fasta
```

* which is simmilar to using `cat` to do this

```
cat my_fasta.fasta | sed 's/\-\-//'g > my_fasta_clean.fasta
```

* when we printed out those bacterial counts before all the filenames made it hard to read the output, lets see if we can correct that now

```
grep -c 'Pseudomonas' HOT_fosmids_small/*/blast_results/*RefSeq*.LASTout.parsed.txt | sed 's/HOT_fosmids_small.*blast_results\///'
below_euphotic_200m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:99
chlorophyllmax_130m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:153
deepabyss_4000m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:208
omz_770m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:182
upper_euphotic_10m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:167
upper_euphotic_70m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:193
uppermesopelagic_500m.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt:190
```

* we can even pass the command through `sed` again to clear the other side of the output, making it much more readable:

```
grep -c 'Pseudomonas' HOT_fosmids_small/*/blast_results/*RefSeq*.LASTout.parsed.txt | sed 's/HOT_fosmids_small.*blast_results\///' | sed 's/.RefSeq_complete_nr_v62_dec2013.LASTout.parsed.txt//'
below_euphotic_200m:99
chlorophyllmax_130m:153
deepabyss_4000m:208
omz_770m:182
upper_euphotic_10m:167
upper_euphotic_70m:193
uppermesopelagic_500m:190
```

* one quick got-ya with `sed` is that by default it only finds one match per line
* you can change it to find all matches per line by enabling 'global matches' by adding a `g` to the end of the three slashes command `'s/old_word/new_word/g'`
    * below we should see out with the `old` text and in with the `new`:

```
echo 'old old old old old old old old' | sed 's/old/new/g'
new new new new new new new new
```

* other thing is the 'print lines' (`p`) option, which will ignore the case of the text when matching and print lines that were substited as a sanity checking procedure.

```
echo 'old old old old new new new' | sed 's/old/new/gp'
```

* One other thing is to switch matches around by containing them in excaped brackets `\(\)` using the `\1`,`\2`, `\3`, and `\4` to refer to the first, second, third and fourth matches, respectively.

```
echo 'one two three four' | sed 's/\(one\)\ \(two\)\ \(three\)\ \(four\)/\4 \3 \2 \1/'
four three two one
```

* We can do this again with with at least one wildcards `+`

```
echo 'one two three four' | sed 's/\(.+\)\ \(.+\)\ \(.+\)\ \(.+\)/\4 \3 \2 \1/'
four three two one
```

* You can also set your particular matching set with `[]`:

```
echo 'one two three four' | sed 's/\([A-z]+\)\ \([A-z]+\)\ \([A-z]+\)\ \([A-z]+\)/\4 \3 \2 \1/'
one two three four
```

* Match a negation with `!` or `^` at the start: `[!abc]` `[^a-c]`

#### awk

Where `sed` was a text replacer, `awk` is more of a text modifier, and in particular is works with tabular data. It can be used to modify files, manipulate columns, or to convert from one tabular file format to another.

```
awk '/search_pattern/ { action_to_take_on_matches; another_action; }' file_to_parse
```

*  one of the first things to do is pass through a file and print out its contents
    * here we take the `functional_and_taxonomic_table.txt` produced by metapathways.

```
awk '{ print }' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
```

* we can use the double-slashes command to isolate certain lines, `'/below_euphotic_200m_7271_0/'`, kind of like `grep`:

``` 
below_euphotic_200m_7271_0    302	2	304	below_euphotic_200m_7271	616	-		all	hypothetical protein
```

* now that we have found our particular lines of interest, we can use action braces `{}` to perform a particular function, like print only the columns that we want using the `print` command and the column references `$1`, `$2`, etc, 
    * here the column needs to contain the word `proteo` in an effort to find all the *proteobacteria*

```
awk '/proteo/ {print $1,"\t", $2, "\t", $3;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
```

* note that the whole line can be printed with `$0`

```
awk '/proteo/ {print $0;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
```

* and we can create our own personal file with a custom column order

```
awk '/proteo/ {print $3,"\t", $2, "\t", $1 ;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
```

* By default the field seperator character `FS` is set to whitespace, but the field character can be specified explicity

```
awk '/proteobacteria/ {FS="\t"; print $1,"\t",$9;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
below_euphotic_200m_7276_0      Gammaproteobacteria
below_euphotic_200m_861_0 	 Gammaproteobacteria
below_euphotic_200m_2785_0 	 Gammaproteobacteria
below_euphotic_200m_3404_0 	 Gammaproteobacteria
below_euphotic_200m_5253_0 	 Gammaproteobacteria
below_euphotic_200m_7506_0 	 Gammaproteobacteria
```

* *Note: It is important in `awk` that you **specify variables with double** (`"`) rather than single quotes (`'`)*

* In fact there are a number of `awk` variables to help with processing the text fields:
    * `FILENAME`: References the current input file.
    * `FS`: The current field separator used to denote each field in a record. By default, this is set to whitespace.
    * `RS`: The record separator used to distinguish separate records in the input file. By default, this is a newline character.
    * `OFS`: The field separator for the outputted data. By default, this is set to whitespace.
    * `ORS`: The record separator for the outputted data. By default, this is a newline character.
    * `NF`: The number of fields in the current record.
    * `NR`: The number of the current record.
    * `FNR`: References the number of the current record relative to the current input file. For instance, if you have two input files, this would tell you the record number of each file instead of as a total.

* For instance, if you specify the `OFS=",", you can quickly turn a tab-delimited file into a custom comma-separated file

```
awk '/proteobacteria/ {FS="\t"; OFS=","; print $1,$9;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
below_euphotic_200m_7276_0,Gammaproteobacteria
below_euphotic_200m_861_0,Gammaproteobacteria
below_euphotic_200m_2785_0,Gammaproteobacteria
below_euphotic_200m_3404_0,Gammaproteobacteria
```

* Note that when this is done, we only need to print the column identifiers rather than exlicity set the separator `print $1,$9;`

* *Note: If you just want to rip-out one of the columns/fields of a file (for instance, to feed into some other program) there's a nice little utility called `cut` to do that
    * the following prints out the ninth-column (taxa) of the functional and taxonomic table

```
cut -f9 HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
```

* There are also `BEGIN` and `END` code blocks that allow you to optionally specify header and footer for the file.

```
awk 'BEGIN { action; }
/search/ { action; }
END { action; }' input_file
```

* For instance it might be helpful to add an informative header to our custom file

```
awk 'BEGIN {FS="\t"; OFS="\t"; print "ORF_ID","Taxonomy"} /proteobacteria/ { print $1,$9;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt
ORF_ID    Taxonomy
below_euphotic_200m_7276_0	Gammaproteobacteria
below_euphotic_200m_861_0	Gammaproteobacteria
below_euphotic_200m_2785_0	Gammaproteobacteria
below_euphotic_200m_3404_0	Gammaproteobacteria
```

* now that we like the structure of our custom file, we are ready to write it out to disk

```
awk 'BEGIN {FS="\t"; OFS="\t"; print "ORF_ID","Taxonomy"} /proteobacteria/ { print $1,$9;}' HOT_fosmids_small/below_euphotic_200m/results/annotation_table/functional_and_taxonomic_table.txt > my_little_tabby.txt
```

* So now the custom file should be read to be read into Excel or R with relatively little additional effort.
* *As a general rule, it is better to use tab characters (`\t`) as delimiters rather than comma characters (`,`) when dealing tables with text fields (like protein annotations), commas in a field will often mess up downstream parsing, an additiona headache we don't really need*

## Unix Shell Scripting

Where the above programs dealt file cleaning and formatting, Unix Shell Scripting really deals with the automation of repetative or simple tasks. In particular, we are going to discuss the `bash` or `b`ourne `a`gain `sh`ell, which is definately the dominant command-line shell in todays unix environment. 

* Most shell scripts involve selecting files with an unix glob `*` and performing an action for each file. This important primative function is setup by creating a `for`-`do`-`done` loop:

```
for file in *fasta
do
    # do some actions
done
```

* As a first example we'll ask the `echo` command to print each filename to the screen, de-referning the filename witht he `$` character

```
for file in *fasta
do
    echo $file
done
```

* since small mistakes when running many commands at once can make a real mess or cause serious damange, **it is often a very good idea to `echo` your loop commands** to inspect them for accuracy before running them for real

* For instance it is very common to rename many files with some kind of suffix

```
for file in *fasta
do
    mv ${file} ${file}.backup
done
```

* This de-references the current file with the `$` or `${}` character and calls the file with the 'move' (`mv`) command to add `.backup` to the name of each file. Especially when appending text to a file the braces `{}` help make the code more clear.

* Furthermore, the `%` character can be used to trim a command form the right-hand-side, usually for changing filenames
    * for instance we will remove the `.backup` suffex we just added to our `.fasta` files
```
for file in *fasta.backup
do
    mv $file ${file%.backup}
done
```

* You can also put a list of things to loop over

```
for i in 1 2 3 4 5
do
    echo $i
done
```

* `while` loops continue while a condition is true, here this multiplies the iterator variable `i` by `10` each time

```
i=0
n=10
while [ $i -le 10 ]
do
  echo "$n * $i = `expr $i \* $n`"
  i=`expr $i + 1`
done
```

##### Variables

* variables have no space (e.g, `myvar=1` is okay, `myvar =1` or `myvar= 1` or `myvar = 1` is not okay)
* `myvar=` or `myvar=""` defines an empty variable
* you can access any local system variable, useful ones are `HOME` and `USERNAME`

##### Larger Shell Scipts

* This is all fine and good but typing commands on the keyboard is error-prone and time consuming, and you can't save your work, so shell scripts can be written in to a file:

* first line of file is `#!<location of bash>` you can find out the location of bash with the `which bash`
* `#` is the comment-out character anything after this character will not be interpreted with exception of the 'sha-bang' character: `#!` 

```
#!/bin/bash

# My first bash script.
echo "Hello World"
```

* this text is in a script called [hello_world.sh](hello_world.sh) for convenience.
* shell scripts are usually run with the `./hello_world.sh` statement when in the same working directory

```
./hello_world.sh
-bash: ./hello_world.sh: Permission denied
```

* What gives? Turns out running a file like this 'executing' needs to have executable permission.

```
ls -la hello_world.sh 
-rw-r--r--  1 nielshanson  staff  55 29 Oct 07:30 hello_world.sh
```

* `-rw-r--r--` specifies the permissions for three specific sets of users the user, the unix group, and others (all users)
    * right now it says that we can read and write to this file, but others can only read it
    * to get executable permissions we'll need to use the `chmod` or `ch`ange `mod`e command

```
chmod 700 hello_world.sh
ls -la hello_world.sh 
-rwx------  1 nielshanson  staff  55 29 Oct 07:30 hello_world.sh
```

* Notice that it now says `-rwx` in the first couple of positions, indicating that we have permission to read (`r`), write (`w`), and execute (`e`) this file.
* Alternatively, we could have used `u+x` instead of `700`, 'add `+` e`x`ecutable permission to `u`ser
* Sometimes this is preferable, because notice that `700` wiped out the groups and other's permissions

```
chmod u+x hello_world.sh
ls -la hello_world.sh 
-rwx------  1 nielshanson  staff  55 29 Oct 07:30 hello_world.sh
```

* One of the major uses for Shell Scripting is to just put a series of commands in order for reference. For instance I've put every command so far in a file called [command_list.sh](command_list.sh)
    * for instance we can run every command we have done so far in the tutoral at once

```
# make sure permissions are right
chmod u+x command_list.sh
./command_list.sh
```

* Extremely common to use shell scripts with `qsub` commands to run on grid computing systems like WestGrid. [run_genovo.sh](run_genovo.sh) is an example of a simple (but very large) job that I ran on grex.westgrid.ca:

```
#!/bin/bash
#PBS -l walltime=165:00:00
#PBS -l pmem=90000mb
/home/nielsh/genovo/assemble /home/nielsh/genovo/test/combined_36m_454_genomes_Sakinaw.fasta 24
```

* notice that they are using comments to send information to the `qsub` command:
    * `-l walltime=160:00:00` says I want to run this script for 165 hours
    * `-l pmem=90000mb` says I want the available process memory to be 90GB
* whenever commands are piggy-backed to send information to another program this is called giving 'directives' (or 'compiler directives')

* here is another good example of a [shell script for running SparCC on WestGrid](https://github.com/hallamlab/utilities/blob/master/SparCC/run_correlation.sh) 
    * *Aria probably has better ways of doing this now*

## Basic Python Scripting

So the Unix Shell and Shell scripting are great for getting a few things done or asking quick questions, however, they are kind of lacking in doing a careful, reproducable analysis. Although, all Unix systems have some version of the above commands, **you really have to check your own system for what works**; based on the particular version. 

Now we are going to do a bit of basic Python scripting which has a lot more capability, and generally a lot more stable accross systems. 

* Python 2.6 or 2.7 are included on most Unix distributions
* Python 3.x is the future of python, and a complete reboot of the language, but right now its not commonly found anywhere by default
    * pick a system and type `python` and its probably referring to some version of Python 2.6 or Python 2.7
    * sometime in the future we may have to [Dive Into Python 3](http://www.diveintopython3.net)

* Python is a whole programming language to master, but for the pourposes of data wrangling we are going to focus on two things:
    * basic file input output (File I/O)
    * writing python scripts

* typing `python` will start an 'interactive shell' and also tell you which python version you are running by default

```
python
Python 2.6.8 (unknown, Jan 23 2013, 20:19:53) 
[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.9.00)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

* this gets you to the python interactive shell which allows you try out some python commands
* good to see if certain packages are installed or if you want to check it something works or behavies quickly
* press `CTRL + c` to type `exit()` to return to the shell

### Basic Script Setup

* lets get outselves going with the basics by writing [hello_world.py]() :

```
#!/usr/bin/python

print "Hello World!"
```

* and run the script as follows:

```
python hello_world.py 
Hello World
```

* lets open a file and print all the lines to the screen:

```
# open the file
file = open("data/functional_and_taxonomic_table.txt", "r")

# read file lines into list
lines = file.readlines()

# iterate through each line and print to screen
for line in lines:
    print line

# close the file
file.close()
```

* there's actually an even more efficent way to do the loop that automatically closes the file

```
with open("data/functional_and_taxonomic_table.txt", "r") as file:
    for line in file:
        print line
```

* well that was fun, but its kind of annoying and error prone to have to specify the file in each way, if only there was some way to specify this from the command line?
* much of Python's functionality operates within Python packages or modules which enable you to do a number of useful tasks. In this tutorial we'll focus on: 
    * `os`: operating system functions
    * `sys`: system functions
    * `re`: regular expressions
    * `argparse`/`optparse`: command-line argument parsers
    * `glob`: Unix-like file selectors (e.g. `*`, `?`, `[]`)
* these can be imported into your python script with the `import` command:

```
import sys
```

* lets rewrite some of our examples step-by-step, this time parsing command line arguments in [basic_script.py](examples/python/basic_script.py)

```
#!/usr/bin/python

# import python packages
import sys

# print the argument vector 
print sys.argv
```

* run this to inspect the contents of `sys.argv`

```
python basic_script.py one two three four
['basic_script.py', 'one', 'two', 'three', 'four']
```

*hey its the stuff from the command line:

We'll use it to open the file that we point it to and print out the lines:

```
#!/usr/bin/python

# import python packages
import sys

# pull filename from the command-line arguments
filename = sys.argv[1]

# open the file 
fh = open(filename, "r")

# read lines from file
lines = fh.readlines()

# iterate through each line and print out content
for line in lines:
    print line

# close the file
fh.close()
```

* now we will run our basic script

```
python basic_script.py data/functional_and_taxonomic_table.txt
```

### Python Parsing Libraries: [optparse](https://docs.python.org/2/library/optparse.html) and [argparse](https://docs.python.org/2/library/argparse.html#module-argparse)

* so normally there is a lot of code that makes sure your scripts look and work great (print help, explain what they do, what options they have, etc.), but fortuantely there are python packages `optparse` and `argparse` which will help you out here. We'll stick with [optparse](https://docs.python.org/2/library/optparse.html) because its older and I know it will work with everyone's machine. However, [argparse](https://docs.python.org/2/library/argparse.html#module-argparse) is better and I provide an example of using it here as well --- but its only available in Python 2.7.x or better.

* here we have the `optparse` version of the script

```
#!/usr/bin/python

# import libraries
import sys # for argument vector
import optparse # to parse arguments

# describe what the script does
what_i_do = "A simple script for printing files"
usage = "myscript.py -i <input_file>"

# initialize the parser
parser = optparse.OptionParser(usage = usage, epilog = what_i_do)
parser.add_option("-i", "--input_file", dest="input_file", default=None,
                   help='file to print out to the screen [Required]')

# the main function of the script
def main():
    (opts, args) = parser.parse_args()
    # opts is a dictionary that contains all the options
    if not opts.input_file:
        print "Error: Input file not specified"
        print usage
        exit()

    # open the file 
    fh = open(opts.input_file, "r")

    # read lines from file
    lines = fh.readlines()

    # iterate through each line and print out content
    for line in lines:
        print line
    
    # close the file
    fh.close()

if __name__ == "__main__":
    main()
```


* when given the `-h` option this prints a little help message to the screen.

```
python myscript.py -h
Usage: myscript.py -i <input_file>

Options:
  -h, --help            show this help message and exit
  -i INPUT_FILE, --input_file=INPUT_FILE
                        file to print out to the screen [Required]

A simple script for printing files
```

* and now the script will have some familiar useage to people

```
python myscript.py -i data/functional_and_taxonomic_table.txt
```

* here's the argparse version (use it if you can) which can be found in [myscript2.py](examples/python/myscript2.py):

```
#!/usr/bin/python

# import libraries
import sys # for argument vector
import argparse # to parse arguments

# describe what the script does
what_i_do = "A simple script for printing files"

# initialize the parser
parser = argparse.ArgumentParser(description=what_i_do)
parser.add_argument("-i", "--input_file", type=str, dest="input_file", default=None,
                   required=True, nargs=1, help='file to print out to the screen [Required]')

# the main function of the script
def main():
    args = vars(parser.parse_args())
    # args is a dictionary that contains all the options
    
    # open the input_file 
    fh = open(args["input_file"], "r")

    # read lines from file
    lines = fh.readlines()

    # iterate through each line and print out content
    for line in lines:
        print line

    # close the file
    fh.close()

if __name__ == "__main__":
    main()
```

* a good thing is that it does useage

```
python2.7 myscript2.py 
usage: myscript2.py [-h] -i [INPUT_FILE]
myscript2.py: error: argument -i/--input_file is required
```

* and error checking

```
python2.7 myscript2.py -i
usage: myscript2.py [-h] -i INPUT_FILE
myscript2.py: error: argument -i/--input_file: expected 1 argument(s)
```

### Parsing File Lines

* Now that we have setup our script, we can now start handling your file line-by-line.
* We'll look at a few function here to help you:
    * `split()` -  split a line into pieces by a character
    * `strip()` - clears whitespace at the end of a line

* isolating our `for` loop

```
for line in lines:
        # split fields by tab
        fields = line.split("\t")
        
        # strip whitespace from the ends of fields
        for i in range(len(fields)):
            fields[i] = fields[i].strip()
```

* take a look at what split does

```
python myscript2.py -i data/functional_and_taxonomic_table.txt
data/functional_and_taxonomic_table.txt
['ORF_ID', 'ORF_length', 'start', 'end', 'Contig_Name', 'Contig_length', 'strand', 'ec', 'taxonomy', 'product\n']
```

* looks like we have a tailing `\n` on the last field, so we'll remove it with `strip("\n")`

```
# do some cleanup
for i in range(len(fields)):
    fields[i] = fields[i].strip()
    fields[i] = fields[i].strip("\n") # strip tailing \n
```

* looks like we fixed the problem

```
python myscript2.py -i data/functional_and_taxonomic_table.txt
['ORF_ID', 'ORF_length', 'start', 'end', 'Contig_Name', 'Contig_length', 'strand', 'ec', 'taxonomy', 'product']
...
```

#### Python Lists and Dictionaries

* two need-to-know data structures in python are the list and the dict or dictionary, and they are invaluable when wrangling a dataset
* both lists and dictionaries are *iteratable* which means that they can be used in for loops to pass through the elements and print them to the screen

##### Lists

We have already been exposed to lists, when we use `readlines()` to get the lines from a file, it gives us a list where each item in the list is a text line.

* They are declared with the square brackets `[]`
* Two common ways to iterate through a list:
    * `item-wise`
    
    ```
    # declare list
    mylist = ['one', 'two', 'three', 'four']
    
    # iterate by item
    for item in mylist:
        print item
    ```
    
    * and `index-wise` via the length `len()` and `range()` function
    
    ```
    # iterate by index
    for i in range(len(mylist)):
        print mylist[i]
    ```

* You can also check for membership in a list with the `in` operator, though this is much slowing than using a dictionary:

```
# check for membership
if 'three' in mylist:
    print "In the list!"
```

* Items can be added to a list by using the `append()` function. For instance, say I wanted to create a list of a particular column of the table we parsed in a previous example

```
taxa = [] # initialize taxa list
for line in lines:
    fields = line.split("\t")
    taxa.append(fields[8])
```

##### Dictionaries

* essentially a sophisticated *hash table*
* a dictionary creates a one-to-one mapping between at set of *keys* and a set of *values*
* it allows for very fast ('amortized constant time') item lookup
* but it doesn't allow duplicates, nor is it ordered like a list

For example, here I'm going to map Open Reading Frame (ORF) id to taxonomy.

```
# example of dictionary
orf_to_taxa = {}
for line in lines:
    fields = line.split("\t")
    if fields[0] not in orf_to_taxa:
        orf_to_taxa[fields[0]] = fields[8]
```

* Here is one of the most common uses of a hash table, tabulating the counts of 

```
taxa_to_count = {}
for line in lines:
    fields = line.split("\t")
    taxa = fields[8] # pull out taxa from current line
    
    # create a place in if taxa new
    if taxa not in taxa_to_count:
        taxa_to_count[taxa] = 0
    
    # add to current taxa count
    taxa_to_count[taxa] = taxa_to_count[taxa] + 1

# quick lookup
if "proteobacteria" in taxa_to_count:
    print "proteobacteria:", taxa_to_count["proteobacteria"] 
```

* One other thing to note is that dictionaries can point to dictionaries or lists recursively, and in this way complex data structures can be built up.
* In theory you can make hierarchical trees (e.g., The NCBI Tree and LCA Rule in MetaPathways)
* For instance a set of dictionaries and lists can be used to implement a simple 2d array (See: [`basic_matrix.py`](examples/python/basic_matrix.py))

* Now that we have processed a few things, we should practice writing things out to disk:
    * use the `open()` function with the `w` parameter to open a filehandle with writing permission
    * write a header for our "Taxa" and "Count" columns
    * iterate through our dictionary constructing a line to write out to disk *(remembering to separate with tab characters `\t` and ending with the newline character`\n`)*
    * write out the line
    * close the file

```
# open a file to write to
out_fh = open(args["output_file"], "w")

# create a header for our file
header = "Taxa" + "\t" + "Count" +"\n"
out_fh.write(header)

# iterate through our taxa count dictionary
for taxa in taxa_to_count:
    out_line = taxa + "\t" + taxa_to_count[taxa] + "\n"
    out_fh.write(out_line)

# close the file
out_fh.close()
```

* lets give it a shot

```
python myscript.py -i data/functional_and_taxonomic_table.txt -o my_taxa_count.txt 
```

##### Parsing a fasta file

* one of the more awkward things it to parse a `fasta` file which has the repetitive pattern. 

```
>below_euphotic_200m_0
CGATGATCACTCCTATAGGGCGAATCGAGCTTTCTCCCGGGGATCCCACCGCATGATTTATGCTAATTACGATTTTTCTACTGAAATCCTCACTGCCTCAGCGCGGGGCCCCATGCACATCTTGGAGGCGGCAAAGATCGGCTCCGATGTTGTGACGGCACC
>below_euphotic_200m_1
CTCTCCTAAATTACTCTAGATCGTCTTCAGTAATATGTCCCCATTCGAACGCCCTTTCTACCCTAGCCTTCGCAGCCTCCGGGTGAAATTGCACCCCCCATGCAGGAAGGGGATATCCCCTTTCATCCAGAACTCTGACTGCTGTGACCCCGTTGTGTACGG
...
```

* Its actually not the most trivial problem because fasta files can have one or many lines of sequences between their header lines (`>`).
* Here we've written a little library `FastaReader.py` to parse and read fasta files, but also how to setup a python library folder (`libs/`)

```
cd libs/
touch __init__.py
```

* `touch __init__.py` creates an empty file `__init__.py` which marks the directory as a place that python should look for library files

* Lets create a new script to handle a fasta file `parse_fasta.py`:
* Next, we add to the top of our script our homemade library

```
import libs.FastaReader
```

* fasta reader takes as input a filename and returns an iterable object that moves its way through the file
* as a first step we'll print out all the sequences in the file

```
fastareader = FastaReader(input_filename)

# print name and record
for record in fastareader:
    print record.name
    print record.sequence
```

#### Python Challenges

* Challenge 1: Write a python script that takes as input a fasta file, compute the GC content (GC characters) / sequence length, and returns tab-delimited file with each sequence name and its GC content in percent.

* Challenge 2 (Harder): Write a function that takes as input a tab-delimited set of network edges ([edges.txt](examples/python/data/edges.txt)). Construct and output a tab-delimited adgacency matrix. 
    * Bonus: If the option `--lower-to-upper` is set, add the lower triangular portion of the matrix to the upper part, creating an upper-triangular matrix with edges in both directions.
    * Extra Bonus: If the option `--viz-network` is set, visualize the network with the Python package [NetworkX](https://networkx.github.io)

#### List of Useful Python Commands/Statements

* `open()`: open a file for reading (`r`) or writing (`w`)
* `close()`: close a file
* `<fh>.readlines()`: reads all lines from an open file in filehandle
* `<fh>.close()`: closes the file
* `<fh>.write()`: write out to an open filehandle with writing permissions
* `with open("file.txt", "r") as file:`: opens a section to handle file, automatically closes file
* `len()`: gets the length of a list object (like command line arguments or line values)
* `sys.argv`: contains a list of all the command-line arguments from the script

## References

Some extra references for more information:

### grep

* [Grep Tutorial](http://www.panix.com/~elflord/unix/grep.html)
* [Using Grep and Regular Expressions to Search for Text Patterns in Linux](https://www.digitalocean.com/community/tutorials/using-grep-regular-expressions-to-search-for-text-patterns-in-linux)

### sed

* [The Basics of Using the Sed Stream Editor to Manipulate Text in Linux](https://www.digitalocean.com/community/tutorials/the-basics-of-using-the-sed-stream-editor-to-manipulate-text-in-linux)
* [UNIX and Linux: Sed Tutorial](http://www.panix.com/~elflord/unix/sed.html)
* [10 examples to replace / delete / print lines of CSV file](http://www.theunixschool.com/2013/02/sed-examples-replace-delete-print-lines-csv-files.html?m=1)

### awk

* [How To Use the AWK language to Manipulate Text in Linux](https://www.digitalocean.com/community/tutorials/how-to-use-the-awk-language-to-manipulate-text-in-linux)
* [An AWK Primer](http://www.vectorsite.net/tsawk.html)

### Shell Scripting

* [Linux Shell Scripting Tutorial](http://www.freeos.com/guides/lsst/) - Great Tutorial! Poor grammar.

### Python

* [Python Code Academy](http://www.codecademy.com/en/tracks/python): Definately a starting point for anyone new to the language.
* [Official Python Tutorial](https://docs.python.org/2/tutorial/index.html):  comprehensive coverage of python topics --- not always super easy to read 
* [Learning Python, 5th Edition](http://shop.oreilly.com/product/0636920028154.do) - Great book for learning python.

### Regular Expressions

* [regular-expressions.info](http://www.regular-expressions.info/)
* [Regular Expression Golf](https://regex.alf.nu): A fun game to get your matching skills up
* [Regular Expression Crossword](http://regexcrossword.com): Quite cool! Definately something to try.
* [Regular Expression Tester](http://ole.michelsen.dk/tools/regex.html): Quick online tool to test regular expressions.
* [Regex One](http://regexone.com): Regular expressions.